'''
å…¨éƒ¨ä»£ç ä½ç½®
2023-8-15
version:1
ç›®æ ‡ï¼š
anki ç‰ˆæœ¬ï¼Œä¿®æ”¹ä¸Šä¼ çš„ç±»å‹

é—®é¢˜ï¼š
åªæœ‰è‹±æ–‡ç¿»è¯‘æƒ…å†µä¸‹ï¼Œæ²¡æœ‰é‡Šä¹‰ï¼›æš‚æ—¶ä¸æ”¹äº†ï¼Œè‡ªå·±åŠ ä¸Šã€‚ä¹Ÿå¥½åŠ ï¼Œä½¿ç”¨is_chinese_soupæ¥åˆ¤æ–­ï¼Œå†çˆ¬è™«å°±æ˜¯äº†
é¡ºåºä¸Šä¼ notionä¸­æ˜¯åçš„ï¼Œç°åœ¨æ”¹æˆåå‘ä¸Šä¼ äº†
å¼€å§‹ä½¿ç”¨gitæ¥ç»´æŠ¤ä»£ç 
'''

# import click
# import notion_api
# import notion_post
# import recognition_word
# import extract_epub

import numpy as np
import cv2
import pyautogui
# import paddleocr
# from PIL import Image
import time
# import pywinauto.mouse
# import ebooklib
# from ebooklib import epub
# from urllib import parse
import pyperclip
import requests
import re
from bs4 import BeautifulSoup
import subprocess
import pickle
import chardet
from datetime import timedelta, date
import datetime
import random
import configparser
import copy
from notion_patch_all_anki import Update_anki
class Economists:
    # ç±»å±æ€§ï¼ˆç±»çº§åˆ«çš„å±æ€§ï¼‰
    # class_attribute = "This is a class attribute"
    # anki https://www.notion.so/4d044f4888104a4c89fbe30487f0198f?v=49f70ebd825a4c85a2a13b9ea10180b8&pvs=4
    # æ„é€ å‡½æ•°ï¼ˆåˆå§‹åŒ–æ–¹æ³•ï¼‰
    def __init__(self):
        # å®ä¾‹å±æ€§ï¼ˆå¯¹è±¡çº§åˆ«çš„å±æ€§ï¼‰
        # self.title = None
        # self.DataBaseAPI = None
        # self.epub_path = "E:/TsinghuaCloud/Seafile/Economist/2023-3-11/TE-2023-03-11-EPUB.epub"
        config = configparser.ConfigParser()
        config.read('token.ini')
        self.token = config.get('token', 'id')
        self.passage_query = config.get('database','passage_query')
        self.database_id = config.get('database', 'anki_query')  # å–databaseå‰è¾¹çš„
        self.guidURL = 'https://dictionary.cambridge.org/dictionary/english-chinese-simplified/'
        self.guidURL_en = 'https://dictionary.cambridge.org/us/dictionary/english/'
        self.headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.0; WOW64; rv:24.0) Gecko/20100101 Firefox/24.0'}
        self.notion_headers = {
            "Authorization": "Bearer " + self.token,
            "accept": "application/json",
            "Notion-Version": "2022-06-28"  # Notionç‰ˆæœ¬å·
        }


        self.words = []
        self.words_to_cambridge = []
        self.repeat_words = []
        self.words_origin = []
        self.sentences = []
        self.title = "060"
        self.my_dict = {}
        # self.my_dict_new = {}
        self.passage_id = {}
        self.passage_num = 0
        self.state = "new"

    def find_middle_sentence_with_phrase(sentence_list, target_phrase):
        # å°†ç›®æ ‡è¯ç»„è½¬æ¢ä¸ºå°å†™ï¼Œä»¥å¿½ç•¥å¤§å°å†™çš„å·®å¼‚
        target_phrase = target_phrase.lower()

        # éå†å¥å­åˆ—è¡¨ï¼Œæ‰¾åˆ°ç‰¹å®šè¯ç»„åœ¨å¥å­ä¸­çš„ç´¢å¼•ä½ç½®
        matching_indices = [index for index, sentence in enumerate(sentence_list) if target_phrase in sentence.lower()]

        # æ‰¾åˆ°ä¸­é—´å¥å­çš„ç´¢å¼•ä½ç½®
        middle_index = len(sentence_list) // 2

        # éå†æ‰¾åˆ°çš„ç´¢å¼•ä½ç½®ï¼Œæ‰¾åˆ°ç¦»ä¸­é—´ä½ç½®æœ€è¿‘çš„ç´¢å¼•
        closest_index = min(matching_indices, key=lambda x: abs(x - middle_index))

        # è¿”å›ä¸­é—´å¥å­çš„ç´¢å¼•ä½ç½®å’Œå¥å­å†…å®¹
        return sentence_list[closest_index]

    def DataBase_item_query(self, query_database_id):
        url_notion_block = 'https://api.notion.com/v1/databases/' + query_database_id + '/query'
        res_notion = requests.post(url_notion_block, headers=self.notion_headers)
        S_0 = res_notion.json()
        res_travel = S_0['results']
        if_continue = len(res_travel)
        if if_continue > 0:
            while if_continue % 100 == 0:
                body = {
                    'start_cursor': res_travel[-1]['id']
                }
                res_notion_plus = requests.post(url_notion_block, headers=self.headers, json=body)
                S_0plus = res_notion_plus.json()
                res_travel_plus = S_0plus['results']
                for i in res_travel_plus:
                    if i['id'] == res_travel[-1]['id']:
                        continue
                    res_travel.append(i)
                if_continue = len(res_travel_plus)
        return res_travel

    def next_day_on_level(self, level):
        if level == '0':
            n = 1
        elif level == '1':
            n = 2
        elif level == '2':
            n = 2
        elif level == '3':
            # n = 3
            n = int(random.randint(3, 5))
        elif level == '4':
            # n = 8
            n = int(random.randint(6, 9))
        elif level == '5':
            # n = 15
            n = int(random.randint(13, 17))
        elif level == '6':
            # n = 30
            n = int(random.randint(28, 32))
        elif level == '7':
            # n = 60
            n = int(random.randint(55, 60))
        elif level == '8':
            # n = 90
            n = int(random.randint(58, 93))
        elif level == '9':
            n = 120
        else:
            n = 1
            print("wrong level")
        return (date.today() + timedelta(days=n)).strftime('%Y-%m-%d')
    def setup(self):
        # å…³é—­clashä»£ç†
        print("å¼€å§‹å…³é—­clashï¼Œè¯·ä¸è¦åŠ¨é¼ æ ‡ã€‚")
        notion_position = pyautogui.locateOnScreen('pictures/clash.png', grayscale=True, confidence= 0.9)
        notion_center = pyautogui.center(notion_position)
        pyautogui.click(notion_center, clicks=1, interval=0.25)
        time.sleep(2)
        notion_position = pyautogui.locateOnScreen('pictures/open.png', grayscale=True, confidence=0.9)
        if notion_position != None:
            notion_center = pyautogui.center(notion_position)
            pyautogui.click(notion_center, clicks=1, interval=0.25)
        time.sleep(1)
        clash_position = pyautogui.locateOnScreen('pictures/clash.png', grayscale=True, confidence=0.9)
        clash_center = pyautogui.center(clash_position)
        pyautogui.click(clash_center, clicks=1, interval=0.25)
        time.sleep(1)
        # pycharm_position = pyautogui.locateOnScreen('pycharm.png', grayscale=True, confidence=0.9)
        # pycharm_center = pyautogui.center(pycharm_position)
        # pyautogui.click(pycharm_center, clicks=1, interval=0.25)
        pass
    # to do
    # def get_today(self):
    #
    #     # è·å–å½“å‰æ—¥æœŸ
    #     current_date = datetime.now().date()
    #
    #     # å°†æ—¥æœŸæ ¼å¼åŒ–ä¸º "YYYY-MM-DD" å½¢å¼
    #     formatted_date = current_date.strftime('%Y-%m-%d')
    #     return formatted_date
    def get_cambridge_soup(self,word_to_search):
        current_guideUrl = self.guidURL
        # self.headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.0; WOW64; rv:24.0) Gecko/20100101 Firefox/24.0'}
        url = current_guideUrl + word_to_search
        source_code = requests.get(url, headers=self.headers).text
        soup = BeautifulSoup(source_code, 'html.parser')
        is_chinese_soup = True
        if len(soup.find_all(class_='def ddef_d db')) != 0:
            return soup,is_chinese_soup
        else:
            is_chinese_soup = False
            current_guideUrl = self.guidURL_en
            url = current_guideUrl + word_to_search
            source_code = requests.get(url, headers=self.headers).text
            soup = BeautifulSoup(source_code, 'html.parser')
            if len(soup.find_all(class_='def ddef_d db')) == 0:
                return [] ,is_chinese_soup
            else:
                return soup ,is_chinese_soup

    def get_cambridge_translation(self, soup, word, is_chinese_soup):
        if len(soup) == 0:
            return [], []
        all = []
        # with open("soup_interest.txt",'w') as file:
        #     file.write(str(soup.prettify()))
        #     file.close()

        # print(soup.prettify())
        # åˆ¤æ–­æ˜¯å¦æœ‰>çš„ï¼Œæ¯”å¦‚ vested interests å’Œ credit
        # phrase-title dphrase-title å¯ä»¥æ‰¾åˆ°å…·ä½“çš„è¯ç»„è‹±æ–‡
        # phrase-body dphrase_b æˆ–  pr phrase-block dphrase-block å¯ä»¥ä¸­è‹±æ–‡é‡Šä¹‰
        # å¾—åˆ°è¿™ä¸ªï¼Œä¸åé¢å¾—åˆ°çš„å¯¹æ¯”å»é‡ï¼Œå¾—åˆ°æœ€ç»ˆçš„ç»“æœ
        phrase = []
        # phrase_title = []
        phrase_translation = []
        phrase_chinese = []
        phrase_english = []
        translation = []

        # pr phrase-block dphrase-block
        if is_chinese_soup:
            phrase_all_span = soup.find_all(
                class_=['pr phrase-block dphrase-block', 'pr phrase-block dphrase-block lmb-25'])
            # print(phrase_all_span)
            if phrase_all_span != None:
                count = 0
                for phrase_all_span in phrase_all_span:
                    current_phase = []
                    count += 1
                    # print(phrase_all_span)
                    # print("+++++++++++++++++++++++++++++++++++++++")
                    phrase_title_span = phrase_all_span.find_all(class_=['phrase-title dphrase-title'])
                    pattern = r'>(.*?)<'
                    phrase_titles_span = re.findall(pattern, str(phrase_title_span))
                    phrase_title = ""
                    # ['', 'and so on', '']
                    for i in phrase_titles_span:
                        # if word in i:
                        #     current_phase.append(i)
                        #     break
                        phrase_title += i
                    print(phrase_title)
                    current_phase.append(phrase_title)
                    # ä¸‹é¢å¼€å§‹æ‰¾é‡Šä¹‰ å…ˆè‹±æ–‡åä¸­æ–‡
                    eng = []
                    chi = []
                    phrase_engs_span = phrase_all_span.find_all(class_=['def ddef_d db'])
                    # print("phrase_engs_span",phrase_engs_span)
                    for phrase_eng_span in phrase_engs_span:
                        pattern = r'>(.*?)<'
                        english_span = re.findall(pattern, str(phrase_eng_span))
                        english_span = ''.join(english_span)
                        if english_span not in phrase_english:
                            eng.append(english_span)
                            phrase_english.append(english_span)
                    # print("eng",eng)

                    chinese_spans = phrase_all_span.find_all(class_='trans dtrans dtrans-se break-cj')
                    # [<span class="trans dtrans dtrans-se break-cj" lang="zh-Hans">æ˜¯â€¦çš„éª„å‚²;æ˜¯â€¦çš„å…‰è£</span>]
                    for chinese_span in chinese_spans:
                        if chinese_span.string not in phrase_chinese:
                            chinese_span_string = chinese_span.string
                            chinese_span_string = chinese_span_string.replace(';','ï¼›')
                            chi.append(chinese_span_string)
                            phrase_chinese.append(chinese_span_string)
                    # print("chi",chi)
                    if len(eng) == len(chi):
                        # for i in range(len(chi)):
                        #     current_phase.append(eng[i])
                        #     current_phase.append(chi[i])
                        current_phase.append(eng)
                        current_phase.append(chi)
                    else:
                        print(eng)
                        print(chi)
                        print("å‡ºç°é”™è¯¯ï¼Œdebugï¼")
                    # print(current_phase)
                    # ['and all', ['and everything else', 'too'], ['ä»¥åŠå…¶ä»–ä¸€åˆ‡ï¼›ç­‰ç­‰', 'ä¹Ÿ']]
                    phrase.append(current_phase)
            # print(phrase)

            # éœ€è¦æ£€æµ‹>ï¼Œéœ€è¦åŒºåˆ†æ¯ä¸ªé‡Šä¹‰æ‰èƒ½å®Œæˆ
            # phrase_spans = soup.find_all(class_='phrase-body dphrase_b')
            # for phrase_span in phrase_spans:
            #     print(phrase_span)
            # phrase.append(phrase_span.string)

            # çˆ¬è™«å…¨éƒ¨ä¸­æ–‡é‡Šä¹‰
            chinese_spans = soup.find_all(class_='trans dtrans dtrans-se break-cj')
            # print(chinese_spans)
            chinese = []
            for chinese_span in chinese_spans:
                chinese_span = chinese_span.string
                chinese_span = chinese_span.replace(';','ï¼›')
                chinese.append(chinese_span)
            # çˆ¬è™«å…¨éƒ¨è‹±æ–‡é‡Šä¹‰
            english_spans = soup.find_all(class_='def ddef_d db')
            english = []
            for english_span in english_spans:
                pattern = r'>(.*?)<'
                english_span = re.findall(pattern, str(english_span))
                english_span = ''.join(english_span)
                english.append(english_span)
                # all[i].append(english)
            if len(chinese) == len(english):
                # for i in range(len(chinese)):
                #     one_translation = []
                #     one_translation.append(word)
                #     one_translation.append(english[i])
                #     one_translation.append(chinese[i])
                #     translation.append(one_translation)
                translation.append(word)
                translation.append(english)
                translation.append(chinese)
            else:
                print(word)
                print("chinese_num != english_numï¼Œä¸­è‹±æ–‡é‡Šä¹‰ä¸å¯¹åº”ï¼Œdebugï¼")
            # all_translation_en = [row[1] for row in translation]
            # print(all_translation_en)
            for one_phrase_english in phrase_english:
                if one_phrase_english in english:
                    i = english.index(one_phrase_english)
                    translation[1].pop(i)
                    translation[2].pop(i)

            print("translation",translation)
            print("phrase",phrase)
            # æœ€ç»ˆç»“æœæ˜¯translation å’Œ phrase translationï¼Œå¦‚æœæ²¡æœ‰phraseï¼Œåè€…å°±æ˜¯ç©ºçš„
            return translation, phrase
        else:
            translation.append(word)
            translation.append("")
            translation.append("")
            return translation, phrase
    def get_cambridge_origin_pronoun_voice(self,soup, flag = 1):
        if len(soup) == 0:
            return "","",""
        # åŸè¯è¡Œè·å–
        #<span class="hw dhw">normalize</span>
        # origin_spans = soup.find_all(class_='hw dhw')
        origin_spans = soup.find_all(class_='tb ttn')
        if len(origin_spans) != 0:
            for origin_span in origin_spans:
                origin_pattern = r'>(.*?)<'
                origin = re.findall(origin_pattern, str(origin_spans))
                if origin != None:
                    break
        else:
            return "","",""
        # print(origin[0])

        # éŸ³æ ‡è·å–
        # <span class="ipa dipa lpr-2 lpl-1">ËˆnaÉª.sÉ™.ti</span>
        # us_pronouns = soup.find_all(class_ = "i i-volume-up c_aud htc hdib hp hv-1 fon tcu tc-bd lmr-10 lpt-3 fs20 hv-3")#,title_ = "Escucha la pronunciaciÃ³n en inglÃ©s americano")
        pronoun_spans = soup.find_all(class_='ipa dipa lpr-2 lpl-1')
        # print(pronoun_spans)
        count = 0
        pronounciation = ""
        pronounciation_us = ""
        pronounciation_uk = ""
        for pronoun_span in pronoun_spans:
            pronoun_pattern = r'>(.*?)<'
            pronoun_gets = re.findall(pronoun_pattern, str(pronoun_span))
            res = ""
            for pronoun_get in pronoun_gets:
                if len(pronoun_get) == 0:
                    continue
                res += pronoun_get
            count += 1
            if count == 1:
                pronounciation_uk += 'UK  /' + res + '/'
                pass
            else:
                pronounciation_us += '/' + res + '/'
            if len(pronounciation_us) != 0 and count == 2:
                break
        pronounciation = pronounciation_us
        if '//' in pronounciation or len(pronounciation) == 0:
            pronounciation = pronounciation_uk
        if '//' in pronounciation or len(pronounciation) == 0:
            pronounciation = ""
        # print(pronounciation)

        # voiceç½‘å€è·å–
        voice_spans = soup.find_all('source', type='audio/mpeg')
        # print("voice_up_spans",voice_up_spans)
        pattern = re.compile(r'src="([^"]+\.mp3)"')
        url_voice = ""
        for voice_span in voice_spans:
            match = pattern.search(str(voice_span))
            if match:
                extracted_voice = match.group(1)
                if "us_pron" in extracted_voice:
                    url_voice = "https://dictionary.cambridge.org" + extracted_voice
                    break
                    pass
        # print(url_voice)
        return origin[0], pronounciation, url_voice
        pass
        # return translation, phrase_translation

    # å®ä¾‹æ–¹æ³•
    def notion_scrap(self): # è¯ç»„å’Œå•è¯åŒºåˆ†çš„ä»»åŠ¡éš¾ä»¥å®Œæˆï¼Œæ”¹ç”¨è°ƒç”¨å‰ªåˆ‡ç‰ˆçš„æ–¹å¼è·å¾—å•è¯ä¿¡æ¯
        print("å¼€å§‹æ‰§è¡Œ")
        time.sleep(5)
        pos = []
        scores = []
        while(True):
            #æˆªå±å¹¶è¯»å–
            im1 = pyautogui.screenshot('pictures/my_screenshot.png')
            img = cv2.imread("pictures/my_screenshot.png")
            # è½¬åˆ°HSV
            hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
            # è®¾ç½®é˜ˆå€¼
            # l_blue = np.array([[0, 43, 46]])
            # h_blue = np.array([25, 255, 255])

            l_blue = np.array([5, 50, 50])  # æ©™è‰²çš„ä¸‹é™
            h_blue = np.array([15, 255, 255])  # æ©™è‰²çš„ä¸Šé™
            # æ„å»ºæ©æ¨¡
            mask = cv2.inRange(hsv, l_blue, h_blue)
            # è¿›è¡Œä½è¿ç®—
            res = cv2.bitwise_and(img, img, mask=mask)
            cv2.imwrite("pictures/result.png", res)
            # å¼€å§‹ocr
            # æ¨¡å‹è·¯å¾„ä¸‹å¿…é¡»å«æœ‰modelå’Œparamsæ–‡ä»¶ï¼Œå¦‚æœæ²¡æœ‰ï¼Œç°åœ¨å¯ä»¥è‡ªåŠ¨ä¸‹è½½äº†ï¼Œä¸è¿‡æ˜¯æœ€ç®€å•çš„æ¨¡å‹
            # use_gpu å¦‚æœpaddleæ˜¯GPUç‰ˆæœ¬è¯·è®¾ç½®ä¸º True
            ocr = paddleocr.PaddleOCR(use_angle_cls=True, use_gpu=False)
            img_path = "pictures/result.png"  # è¿™ä¸ªæ˜¯è‡ªå·±çš„å›¾ç‰‡ï¼Œè‡ªè¡Œæ”¾ç½®åœ¨ä»£ç ç›®å½•ä¸‹ä¿®æ”¹åç§°
            result = ocr.ocr(img_path, cls=True)

            img_res = cv2.imread("pictures/result.png")
            height_img, width_img, _ = img_res.shape
            # print(height_img, width_img)
            for lines in result:
                for line in lines:
                    print("pos", line[0])
                    pos.append(line[0])
                    print("word + scores", line[1])
                    #è¦è§£å†³è¯ç»„è¯†åˆ«ç‡æ¯”å•è¯ä½çš„æƒ…å†µ
                    if ' ' in line[1][0]:
                        if line[1][1] >= 0.93:
                            self.words.append(line[1][0])
                    else:
                        if line[1][1] >= 0.93:
                            self.words.append(line[1][0])

                    # scores.append(line[1][1])
            #è·å¾—äº†word,å†æ¬¡æˆªå±ï¼Œå¦‚æœå˜åŒ–æ²¡è¶…è¿‡é˜ˆå€¼ï¼Œå°±è®¤ä¸º
            if 'This article was downloaded' in self.words:
                new_words = list(filter(lambda x: x != 'This article was downloaded', self.words))
                # print(new_words)
                # print("new_words",len(new_words),new_words)

                # å¤„ç†è¯†åˆ«å‡ºç©ºæ ¼çš„å•è¯
                for num in range(len(new_words)):
                    new_words[num] = new_words[num].strip()
                self.words = list(set(new_words))
                print("words",len(self.words),self.words)
                # cv2.imshow("res",img_res)
                # cv2.waitKey()
                break
            pyautogui.click(800,800, clicks=1, interval=0.25)
            pywinauto.mouse.scroll((800,300),-2) #(1100,300)æ˜¯åˆå§‹åæ ‡ï¼Œ1000æ˜¯æ»‘åŠ¨è·ç¦»ï¼ˆå¯è´Ÿï¼‰
        return self.words
        pass
    def new_dict(self):
        url_notion_block = 'https://api.notion.com/v1/databases/' + self.database_id + '/query'
        res_notion = requests.post(url_notion_block, headers=self.notion_headers)
        S_0 = res_notion.json()
        res_travel = S_0['results']
        if_continue = len(res_travel)
        if if_continue > 0:
            while if_continue % 100 == 0:
                body = {
                    'start_cursor': res_travel[-1]['id']
                }
                res_notion_plus = requests.post(url_notion_block, headers=self.notion_headers, json=body)
                S_0plus = res_notion_plus.json()
                res_travel_plus = S_0plus['results']
                for i in res_travel_plus:
                    if i['id'] == res_travel[-1]['id']:
                        continue
                    res_travel.append(i)
                if_continue = len(res_travel_plus)
        self.my_dict = {}
        count = 0
        for dict in res_travel:
            try:
                self.my_dict[dict['properties']['words']['title'][0]['plain_text']] = dict['id']
                count += 1
                print(count,dict['properties']['words']['title'][0]['plain_text'])
            except:
                print(dict)
                continue
        print("count", count)
        # with open('vocabularies.data', 'wb') as file:
        #     pickle.dump(words, file)
        # with open('vocabularies.data', 'rb') as file:
        #     my_dict = pickle.load(file)

        # for dict_num in range(len(self.my_dict.keys())):
        #     # if ' ' == self.my_dict[dict_num][0] or ' ' == self.my_dict[dict_num][len(self.my_dict[dict_num]) - 1]:
        #     self.my_dict[dict_num] = self.my_dict[dict_num].strip()
        #     self.my_dict[dict_num] = self.my_dict[dict_num].replace('\n', '')
        #     print(self.my_dict[dict_num])
        with open('vocabularies.data', 'wb') as file:
            pickle.dump(self.my_dict, file)

    def get_words_txt(self):

        with open("words.txt", "rb") as file:
            raw_data = file.read()
            result = chardet.detect(raw_data)

        # ä½¿ç”¨æ£€æµ‹åˆ°çš„ç¼–ç æ¥æ‰“å¼€æ–‡ä»¶
        self.detected_encoding = result["encoding"]
        # with open("filename.txt", "r", encoding=detected_encoding) as file:

        # with open("words.txt", "r", encoding="utf-8") as file:
        with open("words.txt", "r", encoding="utf-8") as file:
            words_clip = file.readlines()
        for num in range(len(words_clip)):
            words_clip[num] = words_clip[num].replace('\n','')


        with open("words_to_cambridge.txt", "r", encoding="utf-8") as file:
            words_to_cambridge = file.readlines()
        for num in range(len(words_to_cambridge)):
            words_to_cambridge[num] = words_to_cambridge[num].replace('\n','')

        assert len(words_to_cambridge) == len(words_clip)
        for num in range(len(words_to_cambridge)):
            if words_to_cambridge[num] != "0":
                self.words_to_cambridge.append(words_to_cambridge[num])
                self.words.append(words_clip[num])
                print(words_to_cambridge[num], words_clip[num])

    def get_clip_passage(self):
        pyperclip.copy('')
        # åˆå§‹ä¸Šä¸€æ¬¡çš„å‰ªåˆ‡æ¿å†…å®¹ä¸ºç©º
        prev_clipboard_content = ''
        with open("passage.txt", 'w', encoding='utf-8') as file:
            file.truncate()
        input("æ–‡æ¡£å·²ç»æ¸…ç©ºï¼Œè¯·é€‰ä¸­å¹¶å¤åˆ¶æœ¬æ¬¡è¦å¯¼å…¥å•è¯çš„notionä¸­çš„æ–‡ç« å…¨éƒ¨ï¼Œç„¶åenterå¼€å§‹è¿è¡Œ")
        while True:
            # è·å–å‰ªåˆ‡æ¿å†…å®¹
            # time.sleep(5)
            clipboard_content = pyperclip.paste()
            clipboard_content = clipboard_content.strip()
            # # å¦‚æœå‰ªåˆ‡æ¿å†…å®¹å‘ç”Ÿå˜åŒ–ä¸”ä¸ä¸ºç©ºï¼Œåˆ™å†™å…¥åˆ°txtæ–‡ä»¶ä¸­
            # if clipboard_content != prev_clipboard_content and clipboard_content and clipboard_content not in self.words:
            print(clipboard_content)
            # with open("all_passage.txt", 'r', encoding='utf-8') as file:
            #     file.write(clipboard_content)
            with open("passage.txt", 'w', encoding='utf-8') as file:
                file.write(clipboard_content)
            # self.words.append(clipboard_content)
            # æ›´æ–°ä¸Šä¸€æ¬¡çš„å‰ªåˆ‡æ¿å†…å®¹ä¸ºå½“å‰å†…å®¹
            # prev_clipboard_content = clipboard_content
            resp = input("æ–‡ç« å…¨éƒ¨å¤åˆ¶äº†ï¼Ÿ ä¸æ˜¯æ‰“0")
            if resp != "0":
                break
            # æ¯éš”ä¸€ç§’é’Ÿæ£€æŸ¥ä¸€æ¬¡å‰ªåˆ‡æ¿å†…å®¹
            time.sleep(1)
        subprocess.run(['notepad.exe', "passage.txt"], check=True)

    def get_clip(self):
        pyperclip.copy('')
        # åˆå§‹ä¸Šä¸€æ¬¡çš„å‰ªåˆ‡æ¿å†…å®¹ä¸ºç©º
        prev_clipboard_content = ''
        with open("words.txt", 'w',encoding='utf-8') as file:
            file.truncate()
        with open("words_to_cambridge.txt", 'w',encoding='utf-8') as file:
            file.truncate()
        print("æ£€æŸ¥clashæ˜¯å¦å…³é—­")
        print("æ–‡æ¡£å·²ç»æ¸…ç©ºï¼Œå¼€å§‹è¿è¡Œã€‚\n"
              "ä¾æ¬¡é€‰ä¸­ä½ è¦å¯¼å…¥çš„ç”Ÿè¯æˆ–è¯ç»„ï¼Œç„¶åå¤åˆ¶ï¼š\n")
        print("å¤åˆ¶æ•°å­—20å³ç»“æŸå¤åˆ¶\n")
        words_clip = []
        while True:
            # è·å–å‰ªåˆ‡æ¿å†…å®¹
            clipboard_content = pyperclip.paste()
            clipboard_content = clipboard_content.strip()
            clipboard_content = clipboard_content.replace('\n', '')
            clipboard_content = clipboard_content.strip()
            if clipboard_content == "20":
                print("åœæ­¢æ£€æµ‹å•è¯")
                break
            # å¦‚æœå‰ªåˆ‡æ¿å†…å®¹å‘ç”Ÿå˜åŒ–ä¸”ä¸ä¸ºç©ºï¼Œåˆ™å†™å…¥åˆ°txtæ–‡ä»¶ä¸­
            if clipboard_content != prev_clipboard_content and len(clipboard_content) != 0 and clipboard_content not in words_clip:
                words_clip.append(clipboard_content)
                with open("words.txt", 'a',encoding='utf-8') as file:
                    file.write(clipboard_content + '\n')
                    print(clipboard_content)
                with open("words_to_cambridge.txt", 'a', encoding='utf-8') as file:
                    file.write(clipboard_content + '\n')
                    # self.words.append(clipboard_content)
                # æ›´æ–°ä¸Šä¸€æ¬¡çš„å‰ªåˆ‡æ¿å†…å®¹ä¸ºå½“å‰å†…å®¹
                prev_clipboard_content = clipboard_content

            # æ¯éš”ä¸€ç§’é’Ÿæ£€æŸ¥ä¸€æ¬¡å‰ªåˆ‡æ¿å†…å®¹
            time.sleep(1)
        # myinput = input("å®Œæˆå‰ªåˆ‡æ¿è°ƒç”¨ï¼Œè¯·æ£€æŸ¥æ–‡æ¡£å†…å®¹å¹¶ä¿®æ­£")

        # words = []
        words_to_cambridge = []
        # for word in words_txt:
        #     word = word.replace('\n','')
        #     word = word.strip()
        #     if word not in self.words and len(word) != 0:
        #         words.append(word)
        # æ‰“å¼€txtï¼Œè¿›è¡Œä¿®æ”¹ï¼Œæ¯”å¦‚å°†åŠ¨è¯è¿˜åŸç­‰
        print("å³å°†æ‰“å¼€åŒ…å«é€‰ä¸­çš„å•è¯çš„txtï¼Œå¯æ‰‹åŠ¨å°†å•è¯ä¿®æ”¹ä¸ºåŸå‹ï¼Œå°†è¯ç»„ä¿®æ”¹ä¸ºæ›´å®¹æ˜“æŸ¥åˆ°çš„å½¢å¼\n")
        time.sleep(3)
        subprocess.run(['notepad.exe',"words_to_cambridge.txt"],check = True)
        with open("words_to_cambridge.txt","r") as file:
            words_txt = file.readlines()
        for word in words_txt:
            word = word.replace('\n','')
            word = word.strip()
            if word not in words_to_cambridge and len(word) != 0:
                words_to_cambridge.append(word)
        assert len(words_to_cambridge) == len(words_clip)
        for num in range(len(words_to_cambridge)):
            if words_to_cambridge[num] != "0":
                self.words_to_cambridge.append(words_to_cambridge[num])
                self.words.append(words_clip[num])
                print(words_to_cambridge[num], words_clip[num])




        with open("words.txt", "rb") as file:
            raw_data = file.read()
            result = chardet.detect(raw_data)

        # ä½¿ç”¨æ£€æµ‹åˆ°çš„ç¼–ç æ¥æ‰“å¼€æ–‡ä»¶
        self.detected_encoding = result["encoding"]

    def web_insert_solution(self,sentence):
        while True:
            have_web = False
            if "[" in sentence and ']' in sentence and "(" in sentence and ")" in sentence and "http" in sentence and 'jpg' not in sentence and "png" not in sentence:
                have_web = True
            if have_web == False:
                return sentence
            else:
                print(sentence)
                # [:1]:2(:3):4 å…ˆæ‰¾](ï¼Œå†æ‰¾å‰è¾¹çš„ï¼›å‰è¾¹çš„[å¯èƒ½ä¸å­˜åœ¨ï¼Œé‚£å°±å–0
                index_2 = 0
                index_3 = 0
                index_4 = 0
                for num in range(len(sentence)-1):
                    if sentence[num] == "]" and sentence[num+1] == "(":
                        index_2 = num
                        index_3 = num + 1
                    if index_2 == index_3 - 1:
                        if sentence[num] == ")":
                            index_4 = num
                            break
                for num in range(index_2, -1, -1):
                    if sentence[num] == "[":
                        index_1 = num
                        break
                sentence = sentence[0:index_1] + sentence[index_1+1:index_2] + sentence[index_4+1:len(sentence)]
                print(sentence)






    def get_sentences(self):
        # with open("words.txt","r",encoding='utf-8') as file:
        #     words = file.readlines()
        # words = [item.replace('\n', '') for item in words]
        # self.words = words

        # with open("passage.txt", "r", encoding='utf-8') as bookFile:
        #     paragraphs = bookFile.readlines()
        lines = []
        # copy notion to txt, use this
        with open("passage.txt", "rb") as file:
            # lines = file.readlines()
            for line in file:
                lines.append(line.strip())
        paragraphs = []
        for line in lines:
            try:
                passage =line.decode('utf-8')  # å°è¯•ç”¨utf-8è§£ç 
                paragraphs.append(passage)
                # paragraphs.append(paragraph)
            except UnicodeDecodeError:
                # å¦‚æœutf-8è§£ç å¤±è´¥ï¼Œåˆ™ä½¿ç”¨latin-1è§£ç 
                passage=line.decode('latin-1', errors='replace')  # æ›¿æ¢æ— æ³•è§£ç çš„å­—ç¬¦
                paragraphs.append(passage)
                # paragraphs.append(paragraph)
        # paragraphs = passage.split('\r\r')


        #åˆ†å¥
        sentences_all = []
        for paragraph in paragraphs:
            dot_index = [0]
            if ".jpeg" in paragraph or "èœå•" in paragraph or ".mp3" in paragraph:
                paragraph= re.sub(r'\.mp3|\.jpeg', ' ', paragraph)
                paragraph = re.sub(r'[^\x00-\x7F]+|\|', ' ', paragraph)

                # sentences_all[sen_num] = re.sub(r'[^a-zA-Z0-9]|', ' ', sentences_all[sen_num])
                continue

            paragraph = paragraph.replace("\r\n","")
            if "." not in paragraph or " " not in paragraph:
                if len(paragraph) != 0:
                    sentences_all.append(paragraph)
                    continue
            for alpha_num in range(len(paragraph)):
                if alpha_num != 0:
                    if paragraph[alpha_num-1] == "." and paragraph[alpha_num] == " ":
                        dot_index.append(alpha_num)
                    elif paragraph[alpha_num-2] == "." and paragraph[alpha_num-1] == ")":
                        dot_index.append(alpha_num)
                    elif paragraph[alpha_num-2] == "." and paragraph[alpha_num-1] == "\"":
                        dot_index.append(alpha_num)
                    else:
                        pass
            for dot_num in range(len(dot_index)-1):
                sentences_all.append(paragraph[dot_index[dot_num]:dot_index[dot_num + 1]])
            sentences_all.append(paragraph[dot_index[len(dot_index)-1]:len(paragraph)-1])


            # å»é™¤* å’Œ # å’Œ (http)
        for sen_num in range(len(sentences_all)):
            sentence = sentences_all[sen_num]
            # if "long-planned" in sentences_all[sen_num] :
            #     pass
            sentences_all[sen_num] = sentences_all[sen_num].replace(u'\xa0', ' ')
            sentences_all[sen_num] = sentences_all[sen_num].replace('\n', '')
            sentences_all[sen_num] = sentences_all[sen_num].replace('\r', '')
            sentences_all[sen_num] = sentences_all[sen_num].strip('*# ')
            sentences_all[sen_num] = self.web_insert_solution(sentences_all[sen_num])
            # sentences_all[sen_num] = re.sub(r'\((.*?)\)|\[([^\]]+)\]', lambda m: m.group(2) if m.group(2) else "", sentences_all[sen_num] )
            # sentences_all[sen_num] = sentences_all[sen_num].strip('*')
            # sentences_all[sen_num] = sentences_all[sen_num].strip()
            # sentences_all[sen_num] = sentences_all[sen_num].strip('#')
            # sentences_all[sen_num] = sentences_all[sen_num].strip()
            # sentences_all[sen_num] = sentences_all[sen_num].strip('*')
            # sentences_all[sen_num] = sentences_all[sen_num].strip()
            # a = sentences_all[sen_num]
        for word_num in range(len(self.words)):
            sentences_contain_word = []

            current_word = self.words[word_num].lower()
            # if word_num == 5:
            #     print("main")
            if ' ' in current_word or '-' in current_word: # è¯ç»„
                # count = 0
                for sentence in sentences_all:
                    sentence_lower = sentence.lower()
                    # count += 1
                    # if count > 50:
                    #     print(">50")
                    if current_word in sentence_lower:
                        sentences_contain_word.append(sentence)
                            # find_true_sentence = True
                            # break

                                # if '\n' in sentence:
                                #     sentence = sentence.replace('\n', '')
                                # if '.' in sentence:
                                #     self.sentences.append(sentence)
                                # else:
                                #     self.sentences.append(sentence + '.')
            else:
                # if find_true_sentence:
                #     break
                for sentence in sentences_all:
                    sentence_lower = sentence.lower()
                    sentence_lower = re.sub(r'[^\w\s]', '', sentence_lower)
                    if current_word in sentence.lower():
                        if current_word in sentence_lower.split(' '):
                            sentences_contain_word.append(sentence)
                            continue
                            # find_true_sentence = True
                            # break
                        else:
                            split_result = re.split(r'[-\sâ€”]+', sentence)
                            res_lower = [item.lower() for item in split_result]
                            if current_word in res_lower:
                                sentences_contain_word.append(sentence)
                            continue
                            #
                            # for sentence_line in sentence_lines:
                            #     sentence_space = sentence_line.split(" ")
                            #     if current_word in sentence_space:
                            #         sentences_contain_word.append(sentence)
                            #         continue
                                    # find_true_sentence = True
                                    # break
            assert len(sentences_contain_word) > 0

            sentences_list = ""
            for sentence_contain_word in sentences_contain_word:
                if '\n' in sentence_contain_word:
                    sentence_contain_word = sentence_contain_word.replace('\n', '')
                if '\r' in sentence_contain_word:
                    sentence_contain_word = sentence_contain_word.replace('\r', '')
                if '.' not in sentence_contain_word:
                    sentence_contain_word += '.'
                sentences_list += sentence_contain_word + "\n\n"

            self.sentences.append(sentences_list)
                        # print(self.words[word_num])
                        # print(len(self.sentences))
        # for num in len(self.sentences):
        #     print(self.words[num],self.sentences[num])

    def get_passage_id(self):
        word_passage_num = self.title.split(" ")[0]
        passage_response = self.DataBase_item_query(self.passage_query)
        self.passage_num = len(passage_response)
        for dict in passage_response:
            title_all = dict['properties']['Name']['title'][0]['text']['content']
            if word_passage_num in title_all:
                self.passage_id = dict['id']
                break
        assert len(self.passage_id) > 0

    def get_cambridge(self):
        translations = []
        error_words = []
        origin_pronoun_voice = []
        for word in self.words_to_cambridge:
            current_soup, is_chinese_soup = self.get_cambridge_soup(word)
            origin, pronounciation, voice = self.get_cambridge_origin_pronoun_voice(current_soup)
            self.words_origin.append(origin)
            # print(origin,pronounciation)
            translation, phase_translation = self.get_cambridge_translation(current_soup,word,is_chinese_soup)
            # print(translation,phase_translation)

            if origin == "" or pronounciation == "" or translation == "":
                error_words.append(word)
                continue
            ori_pro_voi = []
            current_translations = []
            ori_pro_voi.append(origin)
            ori_pro_voi.append(pronounciation)
            ori_pro_voi.append(voice)
            origin_pronoun_voice.append(ori_pro_voi)
            current_translations.append(word)
            current_translations.append(ori_pro_voi)
            current_translations.append(translation)
            current_translations.append(phase_translation)
            translations.append(current_translations)
        # for i in translations:
        #     print(i)
        # print(origin_pronoun, error_words)
        print("translations",translations)
        return origin_pronoun_voice,translations, error_words

    def notion_post(self, word_tag,word_color,word_content,meaning,pronoun,sentence,voice_url):
        today_str = date.today().strftime('%Y-%m-%d')
        next_str = (date.today() + timedelta(days=1)).strftime('%Y-%m-%d')
        url = "https://api.notion.com/v1/pages"
        p = {"parent": {"database_id": self.database_id},
             "properties": {
                 "Tags": {"select": {"name": word_tag, "color": word_color}},
                 "words": {"title": [{"type": "text", "text": {"content": word_content}}]},
                 "passage": {"multi_select": [{"name": self.title}]},
                 "phonetic symbol": {"rich_text": [{"type": "text", "text": {"content": pronoun}}]},
                 "meaning": {
                     "rich_text": [{"type": "text", "text": {"content": meaning}}]},
                 "Last": {"date": {"start": today_str}},
                 "Next": {"date": {"start": next_str}},
                 "voice": {"url": voice_url},
                 "Level": {"select": {"name": "0", "color": "default"}},
                 "KnowAll": {"checkbox": False},
                 "KnowSome": {"checkbox": False},
                 "ForgetAll": {"checkbox": False},
                 "Checked Times": {"number": 0},
                 "ğŸŒ Economist Reading": {
                    "relation": [
                        {
                            "id": self.passage_id
                        }
                    ]
                    },
             },

             "children": [
                 {
                     "type": "heading_3",
                     "heading_3": {
                         "rich_text": [{
                             "type": "text",
                             "text": {
                                 "content": sentence,
                             }
                         }],
                         "color": "default",
                         "is_toggleable": False,
                     }

                 }
             ],
             }
        headers = {
            "Notion-Version": "2022-06-28",
            "Content-Type": "application/json",
            "Authorization": "Bearer " + self.token
        }
        r = requests.post(url, json=p, headers=headers)
        print(r.text)
        if r.status_code == 200:
            print("å¯¼å…¥NotionæˆåŠŸï¼" + word_content)
        else:
            print("å¯¼å…¥Notionå¤±è´¥ï¼")

    def repeat_patch(self):
        origin_data = {
            "parent": {"type": "database_id", "database_id": self.database_id},
            "properties": {
                # "Tags": {"select": {"name": word_tag, "color": word_color}},
                # "words": {"title": [{"type": "text", "text": {"content": word_content}}]},
                # "passage": {"multi_select": [{"name": self.title}]},
                "passage": {"multi_select": [{"name":self.title}]},
                # "phonetic symbol": {"rich_text": [{"type": "text", "text": {"content": pronoun}}]},
                # "meaning": {
                #     "rich_text": [{"type": "text", "text": {"content": meaning}}]},
                # "Last": {"date": {"start": today_str}},
                # "Next": {"date": {"start": next_str}},
                # "voice": {"url": voice_url},
                # "Level": {"select": {"name": "0", "color": "default"}},
                # "KnowAll": {"checkbox": False},
                # "KnowSome": {"checkbox": False},
                # "ForgetAll": {"checkbox": False},
                # "Checked Times": {"number": 0},
                "ğŸŒ Economist Reading": {
                    "relation": [
                        {
                            "id": self.passage_id
                        }
                    ]
                },
            },
        }
        for repeat_word in self.repeat_words:
            print(repeat_word)
            word_id = self.my_dict[repeat_word]
            url = 'https://api.notion.com/v1/pages/' + word_id
            notion_page = requests.get(url, headers=self.notion_headers)
            result = notion_page.json()
            title_origins = result["properties"]["passage"]["multi_select"]
            data = copy.deepcopy(origin_data)
            for title_origin in title_origins:
                title = title_origin['name']
                if {"name":title} not in data["properties"]["passage"]["multi_select"]:
                    data["properties"]["passage"]["multi_select"].append({"name":title})
            for relation in result["properties"]["ğŸŒ Economist Reading"]["relation"]:
                if relation not in data["properties"]["ğŸŒ Economist Reading"]["relation"]:
                    data["properties"]["ğŸŒ Economist Reading"]["relation"].append(relation)
            # if {"name":self.title} not in data["properties"]["passage"]["multi_select"]:
            #     data["properties"]["passage"]["multi_select"].append({"name":self.title})
            r = requests.patch(
                "https://api.notion.com/v1/pages/{}".format(word_id),
                json=data,
                headers=self.notion_headers,
            )
            print(r.text)


    def run(self):
        while True:
            try:
                source_code = requests.get(self.guidURL + 'and', headers=self.headers).text
                break
            except:
                input("æ— æ³•çˆ¬è™«ï¼Œè¯·å…³é—­vpnï¼›\n å…³é—­åè¯·å›è½¦")
            # self.setup()
        self.title = input("è¾“å…¥è¿™æ¬¡çš„æ–‡ç« æ ‡å·ï¼šå‚è€ƒ063ï¼š\n")
        selection = input("éœ€è¦clipæ–‡ç« å—ï¼Ÿ ä¸éœ€è¦æ‰“0 \n")
        if selection != "0":
            self.get_clip_passage()
        else:
            pass
        selection = input("éœ€è¦clipå•è¯å—ï¼Ÿ ä¸éœ€è¦æ‰“0\n")
        if selection == "0":
            self.get_words_txt()
        else:
            self.get_clip()
        print(len(self.words),self.words)
        input("check it æŒ‰å›è½¦å¼€å§‹çˆ¬è™«\n")
        self.get_sentences()
        origin_pronoun_voice,translations, error_words = self.get_cambridge()
        if len(self.words) != len(self.sentences):
            print("word",len(self.words),"sentences",len(self.sentences))
            print("å•è¯å’Œä¾‹å¥æ•°é‡ä¸å¯¹åº”ï¼Œdebugï¼")
            return None
        print(len(self.words),len(self.sentences))
        with open("words.txt", 'w',encoding=self.detected_encoding) as file:
            file.truncate()
            for w in self.words:
                file.write(w + "\n")
        with open("words_upload.txt", 'w',encoding=self.detected_encoding) as file:
            file.truncate()
        with open("words_repeat.txt", 'w',encoding=self.detected_encoding) as file:
            file.truncate()

        print("word",self.words)
        # print("sentences",self.sentences)
        print("error",error_words)
        is_check = input("è¦ä¸è¦æ£€æŸ¥æœ‰æ²¡æœ‰é‡å¤ ä¸è¦æ‰“0: \n")
        if is_check != "0":
            with open('vocabularies.data', 'rb') as file:
                self.my_dict = pickle.load(file)
        else:
            pass
        # print(self.my_dict)
        print(len(self.my_dict))
        # for dict_num in range(len(self.my_dict)):
        #     # if ' ' == self.my_dict[dict_num][0] or ' ' == self.my_dict[dict_num][len(self.my_dict[dict_num]) - 1]:
        #     self.my_dict[dict_num] = self.my_dict[dict_num].strip()
        #     self.my_dict[dict_num] = self.my_dict[dict_num].replace('\n','')
        #     pass

        translations_num = -1
        chongfu_num = 0
        up_word_tag = []
        up_word_color = []
        up_word_content = []
        up_meaning = []
        up_pronoun = []
        up_sentence = []
        up_voiceUrl = []
        # print(self.my_dict)

        for i in range(len(self.words)):
            current_word = self.words[i]
            if self.words_to_cambridge[i] not in error_words:
                translations_num += 1
                if self.words[i] in self.my_dict or self.words_origin[i] in self.my_dict:
                    chongfu_num += 1
                    if self.words[i] in self.my_dict:
                        self.repeat_words.append(self.words[i])
                    else:
                        self.repeat_words.append(self.words_origin[i])
                    with open("words_repeat.txt","a", encoding='utf-8') as file:
                        current_translation = translations[translations_num]
                        # except:
                        # print(self.words[i])
                        file.write("-------------------- word " + str(i) + " --------------------" + "\n")
                        word_in_passage = current_translation[0]
                        file.write(word_in_passage + "\n")
                        word_origin = current_translation[1][0]
                        file.write(word_origin + "\n")
                        word_pronoun = current_translation[1][1]
                        # file.write(word_pronoun + "\n")
                        voiceUrl = current_translation[1][2]
                        file.write(voiceUrl + "\n")
                        eng_trans = current_translation[2][1]
                        chi_trans = current_translation[2][2]

                        file.write("===== translation" + str(i) + " =====" + "\n")
                        for j in range(len(chi_trans)):
                            file.write(eng_trans[j] + " \n")
                            file.write(chi_trans[j] + " \n")
                        file.write(">>>>> phrase " + str(i) + " <<<<<" + "\n")
                        phrase_all = current_translation[3]
                        if len(phrase_all) != 0:
                            for current_phrase in phrase_all:
                                phrase_word = current_phrase[0]
                                phrase_eng_trans = current_phrase[1]
                                phrase_chi_trans = current_phrase[2]
                                file.write(phrase_word + "\n")
                                for pj in range(len(phrase_chi_trans)):
                                    file.write(phrase_eng_trans[pj] + "\n")
                                    file.write(phrase_chi_trans[pj] + "\n")
                            pass
                        file.write("///// sentences " + str(i) + " /////" + "\n")
                        file.write(self.sentences[i] + "\n")
                    continue
                else:
            # if self.words[i] not in error_words:
            #         translations_num += 1
                    with open("words_upload.txt","a",encoding='utf-8') as file:
                        # file.write(self.words[i] + " " + origin_pronoun + "\n")
                        # file.write(translations[i] + "\n")
                        # file.write(self.sentences[i] + "\n")
                        # print(self.words[i] + " " + origin_pronoun + "\n")
                        # print(translations[i] + "\n")
                        # try:
                        current_translation = translations[translations_num]
                        # except:
                        # print(self.words[i])
                        file.write("-------------------- word "+ str(i)+" --------------------"+ "\n")
                        word_in_passage = current_translation[0]
                        file.write(word_in_passage + "\n")
                        word_origin = current_translation[1][0]
                        file.write(word_origin + "\n")
                        up_word_content.append(word_origin)
                        word_pronoun = current_translation[1][1]
                        voiceUrl = current_translation[1][2]
                        up_pronoun.append(word_pronoun)
                        print("voiceUrl",voiceUrl)
                        up_voiceUrl.append(voiceUrl)
                        # file.write(word_pronoun + "\n")
                        try:
                            eng_trans = current_translation[2][1]
                            chi_trans = current_translation[2][2]
                        except:
                            eng_trans = ""
                            chi_trans = ""

                        file.write("===== translation" + str(i) +" ====="+ "\n")
                        meaning = ""
                        for j in range(len(chi_trans)):
                            file.write(eng_trans[j]+ " \n")
                            file.write(chi_trans[j]+ " \n")
                            meaning += eng_trans[j]+ " \n"
                            meaning += chi_trans[j]+ " \n"
                        file.write(">>>>> phrase " + str(i) + " <<<<<" + "\n")
                        phrase_all = current_translation[3]
                        if len(phrase_all) != 0:
                            meaning += "-----phrase-----\n"
                            for current_phrase in phrase_all:
                                phrase_word = current_phrase[0]
                                phrase_eng_trans = current_phrase[1]
                                try:
                                    phrase_chi_trans = current_phrase[2]
                                except:
                                    phrase_chi_trans = ""
                                file.write(phrase_word + "\n")
                                meaning += phrase_word + "\n"
                                for pj in range(len(phrase_chi_trans)):
                                    file.write(phrase_eng_trans[pj] + "\n")
                                    file.write(phrase_chi_trans[pj] + "\n")
                                    meaning +=phrase_eng_trans[pj] + "\n"
                                    meaning +=phrase_chi_trans[pj] + "\n"
                            pass
                        meaning.rstrip()
                        up_meaning.append(meaning)
                        file.write("///// sentences " + str(i) + " /////" + "\n")
                        file.write(self.sentences[i] + "\n")
                        up_sentence.append(self.sentences[i])
            else:
                if self.words[i] in self.my_dict:
                    chongfu_num += 1
                    self.repeat_words.append(self.words[i])
                    with open("words_repeat.txt","a",encoding='utf-8') as file:
                        file.write("-------------------- error word " + str(i) + " --------------------" + "\n")
                        file.write(self.words[i] + "\n")
                        file.write("///// sentences " + str(i) + " /////" + "\n")
                        file.write(self.sentences[i] + "\n")
                else:
                    up_word_content.append(self.words[i])
                    with open("words_upload.txt","a",encoding='utf-8') as file:
                        file.write("-------------------- error word " + str(i) + " --------------------" + "\n")
                        file.write(self.words[i] + "\n")
                        file.write("///// sentences " + str(i) + " /////" + "\n")
                        file.write(self.sentences[i] + "\n")
                        up_sentence.append(self.sentences[i])
                        up_pronoun.append(" ")
                        up_meaning.append(" ")
                        up_voiceUrl.append(" ")

        if chongfu_num + len(up_meaning) == len(self.words):
            input("éƒ½å¯¹åº”ä¸Šäº†ï¼Œå¯ä»¥enter")
        subprocess.run(['notepad.exe',"words_upload.txt"],check = True)
        subprocess.run(['notepad.exe',"words_repeat.txt"],check = True)
        # input("æ£€æŸ¥result.txtï¼Œå¦‚æœæ²¡æœ‰é—®é¢˜å°±enter")
        # è®¾ç½®tag color
        for content_num in range(len(up_word_content)):
            up_word_content[content_num] = up_word_content[content_num].strip()
            if " " in up_word_content[content_num]:
                up_word_tag.append("è¯ç»„")
                up_word_color.append("pink")
            else:
                up_word_tag.append("å•è¯")
                up_word_color.append("yellow")
        print(len(up_meaning))
        print(len(up_pronoun))
        print(len(up_word_content))
        print(len(up_word_tag))
        print(len(up_word_color))
        print(len(up_voiceUrl))
        if len(up_voiceUrl) != len(up_word_content):
            for opv in origin_pronoun_voice:
                print(opv)
        # today = self.get_today()
        # today = self.get_today()

        self.get_passage_id()

        # å¼€å§‹ä¸Šä¼ notion
        for num in range(len(up_word_content)-1,-1,-1):
            self.notion_post(up_word_tag[num],up_word_color[num],up_word_content[num],up_meaning[num],up_pronoun[num],up_sentence[num],up_voiceUrl[num])
        # for word in self.words:
        #     if word not in self.my_dict:
        #         self.my_dict.append(word)
        self.new_dict()
        selection = input("å‡†å¤‡æ›´æ–°é‡å¤çš„å•è¯çš„tagå’Œrelation ")
        if selection != "0":
            self.repeat_patch()
            print("End, æ­å–œä½ ç²¾è¯»äº†ä¸€ç¯‡æ–‡ç« ï¼Œè¿™æ˜¯æ‚¨çœ‹çš„ç¬¬" + str(self.passage_num) + "ç¯‡")
        else:
            print("End, æ­å–œä½ ç²¾è¯»äº†ä¸€ç¯‡æ–‡ç« ï¼Œè¿™æ˜¯æ‚¨çœ‹çš„ç¬¬" + str(self.passage_num) + "ç¯‡")

        # with open('vocabularies.data', 'wb') as file:
        #     pickle.dump(self.my_dict, file)
    # for word in self.words:
        #     con = True
        #     for sen in self.sentences:
        #         if word in sen:
        #             con = False
        #             break
        #     if con:
        #         print(word)
        # page = cv2.imread("result.png")
        # cv2.imshow("page",page)
        # cv2.waitKey()

if __name__ == "__main__":
    selection = input("æ‚¨å¥½ï¼Œè¯·é€‰æ‹©è¦æ‰§è¡Œçš„å†…å®¹ï¼š\n1. Anki Update\n2. è¯»æ–°çš„æ–‡ç« \n3. test")
    if selection == "1":
        print("start getting response")
        a = Update_anki()
        response = a.DataBase_item_query()
        # response = None
        print("start updating")
        a.patch_update(response)
    elif selection == "2":
        test = Economists()
        test.run()
    else:
        test = Economists()
        test.run()
    # test.new_dict()
    # with open('vocabularies.data', 'rb') as file:
    #     my_dict = pickle.load(file)
    # print(my_dict)
    # soup = test.get_cambridge_soup("suasion")
    # test.get_cambridge_origin_pronoun_voice(soup)
    # test.notion()
    # print(len(test.notion()),)
    # test.get_sentences()

# with open("result.txt", "r") as file:
#     all_content = file.readlines()
#     word_index = []
#     for num in range(len(all_content)):
#         if "--------------------" in all_content[num]:
#             word_index.append[num]
#     for index in word_index:

